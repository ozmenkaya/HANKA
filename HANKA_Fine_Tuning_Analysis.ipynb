{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "335475b4",
   "metadata": {},
   "source": [
    "# HANKA AI - Fine-Tuning Data Analysis\n",
    "\n",
    "This notebook analyzes the training data for HANKA AI fine-tuning. It checks data quality, distribution, and sufficiency to ensure the best results for the fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3381fb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    import seaborn as sns\n",
    "except ImportError:\n",
    "    sns = None\n",
    "    print(\"seaborn not installed, some plots will be simplified\")\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c0d76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and Parse Data\n",
    "# Note: Upload your 'training_data.jsonl' or 'hanka_finetune_dataset.jsonl' file to the Colab environment first.\n",
    "file_path = 'hanka_finetune_dataset.jsonl' \n",
    "\n",
    "records = []\n",
    "errors = []\n",
    "\n",
    "try:\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line_num, line in enumerate(f, 1):\n",
    "            try:\n",
    "                record = json.loads(line.strip())\n",
    "                records.append(record)\n",
    "            except json.JSONDecodeError as e:\n",
    "                errors.append(f\"Line {line_num}: {str(e)}\")\n",
    "    print(f\"Successfully loaded {len(records)} records.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File '{file_path}' not found. Please upload the dataset.\")\n",
    "    # Create dummy data for demonstration if file not found\n",
    "    print(\"Creating dummy data for demonstration...\")\n",
    "    records = [\n",
    "        {\"messages\": [{\"role\": \"user\", \"content\": \"siparişler\"}, {\"role\": \"assistant\", \"content\": \"{\\\"sql\\\": \\\"SELECT * FROM siparisler\\\"}\"}], \"metadata\": {\"firma_id\": 1, \"timestamp\": \"2023-10-27 10:00:00\"}},\n",
    "        {\"messages\": [{\"role\": \"user\", \"content\": \"müşteriler\"}, {\"role\": \"assistant\", \"content\": \"{\\\"sql\\\": \\\"SELECT * FROM musteri\\\"}\"}], \"metadata\": {\"firma_id\": 1, \"timestamp\": \"2023-10-27 11:00:00\"}},\n",
    "        {\"messages\": [{\"role\": \"user\", \"content\": \"stok durumu\"}, {\"role\": \"assistant\", \"content\": \"{\\\"sql\\\": \\\"SELECT * FROM stok\\\"}\"}], \"metadata\": {\"firma_id\": 2, \"timestamp\": \"2023-10-28 09:00:00\"}}\n",
    "    ] * 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3063662a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Structure Analysis\n",
    "df = pd.DataFrame(records)\n",
    "\n",
    "# Helper to extract message content\n",
    "def get_content(messages: list, role: str) -> str | None:\n",
    "    for m in messages:\n",
    "        if m['role'] == role:\n",
    "            return m['content']\n",
    "    return None\n",
    "\n",
    "df['user_content'] = df['messages'].apply(lambda x: get_content(x, 'user'))\n",
    "df['assistant_content'] = df['messages'].apply(lambda x: get_content(x, 'assistant'))\n",
    "df['system_content'] = df['messages'].apply(lambda x: get_content(x, 'system'))\n",
    "\n",
    "print(\"Dataset Sample:\")\n",
    "display(df[['user_content', 'assistant_content']].head())\n",
    "\n",
    "print(f\"\\nTotal Records: {len(df)}\")\n",
    "print(f\"Missing User Content: {df['user_content'].isna().sum()}\")\n",
    "print(f\"Missing Assistant Content: {df['assistant_content'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509d4625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token Count Estimation (Approximate)\n",
    "# OpenAI uses tiktoken, but for a rough estimate we can use character count / 4\n",
    "def estimate_tokens(text: str) -> float:\n",
    "    if not text: return 0\n",
    "    return len(str(text)) / 4\n",
    "\n",
    "df['user_tokens'] = df['user_content'].apply(estimate_tokens)\n",
    "df['assistant_tokens'] = df['assistant_content'].apply(estimate_tokens)\n",
    "df['system_tokens'] = df['system_content'].apply(estimate_tokens)\n",
    "df['total_tokens'] = df['user_tokens'] + df['assistant_tokens'] + df['system_tokens']\n",
    "\n",
    "total_dataset_tokens = df['total_tokens'].sum()\n",
    "estimated_cost_gpt35 = (total_dataset_tokens / 1000) * 0.0080 # Example rate for GPT-3.5 Turbo Fine-tuning\n",
    "\n",
    "print(f\"Average Tokens per Example: {df['total_tokens'].mean():.2f}\")\n",
    "print(f\"Total Dataset Tokens (Approx): {total_dataset_tokens:,.0f}\")\n",
    "print(f\"Estimated Training Cost (1 Epoch, GPT-3.5): ${estimated_cost_gpt35:.4f}\")\n",
    "\n",
    "# Plot Token Distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "if sns:\n",
    "    sns.histplot(df['total_tokens'], bins=30, kde=True)\n",
    "else:\n",
    "    plt.hist(df['total_tokens'], bins=30)\n",
    "plt.title('Token Distribution per Example')\n",
    "plt.xlabel('Token Count')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658de56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL Query Analysis\n",
    "import re\n",
    "\n",
    "def extract_sql(content):\n",
    "    try:\n",
    "        # Content is usually a JSON string like {\"sql\": \"SELECT ...\", \"explanation\": \"...\"}\n",
    "        data = json.loads(content)\n",
    "        return data.get('sql', '')\n",
    "    except:\n",
    "        return ''\n",
    "\n",
    "df['sql_query'] = df['assistant_content'].apply(extract_sql)\n",
    "\n",
    "# Extract Table Names (Simple Regex)\n",
    "def extract_tables(sql):\n",
    "    # Matches FROM table or JOIN table\n",
    "    # Very basic regex, might need refinement for complex queries\n",
    "    tables = re.findall(r'FROM\\s+`?(\\w+)`?|JOIN\\s+`?(\\w+)`?', sql, re.IGNORECASE)\n",
    "    # Flatten list of tuples and filter empty strings\n",
    "    return [t for group in tables for t in group if t]\n",
    "\n",
    "all_tables = []\n",
    "for sql in df['sql_query']:\n",
    "    all_tables.extend(extract_tables(sql))\n",
    "\n",
    "table_counts = pd.Series(all_tables).value_counts()\n",
    "\n",
    "print(\"Top 10 Most Accessed Tables:\")\n",
    "print(table_counts.head(10))\n",
    "\n",
    "# Plot Table Usage\n",
    "plt.figure(figsize=(12, 6))\n",
    "table_counts.head(15).plot(kind='bar')\n",
    "plt.title('Top 15 Most Accessed Tables in Training Data')\n",
    "plt.xlabel('Table Name')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e54246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System Prompt Analysis\n",
    "unique_system_prompts = df['system_content'].unique()\n",
    "print(f\"Unique System Prompts: {len(unique_system_prompts)}\")\n",
    "\n",
    "if len(unique_system_prompts) < 10:\n",
    "    for i, prompt in enumerate(unique_system_prompts):\n",
    "        print(f\"Prompt {i+1}: {prompt[:100]}...\")\n",
    "else:\n",
    "    print(\"Top 5 System Prompts:\")\n",
    "    print(df['system_content'].value_counts().head(5))\n",
    "\n",
    "# Conclusion\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ANALYSIS CONCLUSION\")\n",
    "print(\"=\"*50)\n",
    "if len(df) >= 50:\n",
    "    print(\"✅ Dataset size is sufficient for initial fine-tuning.\")\n",
    "else:\n",
    "    print(\"⚠️ Dataset size is small. Consider collecting more examples (aim for 50-100+).\")\n",
    "\n",
    "if df['total_tokens'].mean() < 2000:\n",
    "    print(\"✅ Token counts are within reasonable limits.\")\n",
    "else:\n",
    "    print(\"⚠️ Some examples are very long. Check for context window limits.\")\n",
    "\n",
    "print(\"\\nReady to upload to OpenAI!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
